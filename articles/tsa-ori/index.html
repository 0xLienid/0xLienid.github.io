<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" href="styles.css">
  </head>
  <body class="page">
    <div class="article">
      <h2>Token Space Attention and Original Representation Interactions</h2>
      <span>

        <h3>Note</h3>
        <p>
          This is not a particularly rigorous or academic writeup. My primary intention behind it is to make
          myself see this quick project through to completion, but secondarily to just put more of my ideas on machine
          learning on paper. For that reason, there's going to be no fancy tables, charts, or diagrams. Just 
          code and screenshots from WandB.
        </p>

        <br />

        <h3>Introduction</h3>
        <p>
          When you think about something, say a cat, your brain immediately conjures up its generic heuristic,
          or latent space representation, for it. When you then think through various actions/manipulations over that cat, by
          imagining it walking, or flipping it upside down, the representation you see in your mind's eye has
          changed, but you still recognize that this is a manipulation over the cat heuristic, it's not some new
          object in and of itself.

        </p>

        <p>
          To rephrase that in the context of language models, you don't lose track of the original token inputs and
          you don't lose track of initial embedding space representation of those tokens as you step through blocks/layers.
        </p>

        <p>
          In a slightly tangential, but similar framing, think about vision for a moment. The totality of the scene
          is processed in a compressed embeddings space where different portions of the scene are thought of only in
          abstract concepts (i.e. the JEPA framing), but you attend to objects in pixel space too. If I stare very
          closely at this document, I see it not just as a large collection of words, in an editor, sitting in my computer screen,
          but rather I can focus on individual elements, and then individual sub-elements, and so on.
        </p>

        <p>
          I think this relationship between raw inputs, initial representations, and manipulated representations as you
          pass through model layers is probably important in some way.
        </p>

        <p>
          One attempt to recognize that previous representations are important was the denseformers architecture that
          released recently. The high-level idea is to have the input to each layer be a weighted linear combination
          of the output of previous layers, all the way back to the original embedding representation of the input.
          The denseformers paper found improvements over the base transformers architecture, but my feeling was that 
          this approach was to some extent overkill and that the primary benefits stem attention over the original 
          input (or the embedding representation of it).
        </p>

        <br />

        <h3>Approach</h3>
        <p>
          I examined two approachs, and one followed from initial struggles with the other.
        </p>

        <p>
          The first is the Token Space Attention (TSA) architecture. The premise is relatively simple, but it's implementation
          is slightly more complex than the second approach. As I mentioned in the introduction, the premise is "what if
          you included some representation of the original input in every attention pass?" To do this I created a new 
          embedding module with an extra dimension that is fixed to the token value (scaled by the max token value so 
          as to not crowd out the descriptiveness of the other embedding dimensions when RMSNorm is taken over it) 
          and is not updated during the learning process. Then, through every attention module, you sum the original 
          embedding representation of the input tokens with the output from the last transformer block.
        </p>
          
        <p>
          The second is the Original Representation Interaction (ORI) architecture. This came into being while playing around 
          with variations of TSA, seeing if there were additional performance gains to be had. ORI has both a simple premise 
          and a simple implementation (so much so that it almost dissuaded me from writing this because it felt trivial). 
          Again, you keep the original embedding representation of the input tokens, but this time you perform an elementwise 
          multiplication with the last block's output before feeding it to the attention module. I'll discuss more about 
          what this means from a more theoretical perspective later.
        </p>

        <p>
          I compared the eval loss of:
          - Llama (164m params)
          - Llama denseformer (164m params)
          - TSA (160m params)
          - TSA denseformer (160m params)
          - ORI (160m params)
          - ORI denseformer (160m params)
          on the TinyStories dataset.
        </p>
        
        <br />

        <h3>Findings</h3>
        <div>
          <img src="assets/wandb_raw_plot.png" alt="wandb_raw_plot" width="450px" />
          <img src="assets/wandb_plot_with_values.png" alt="wandb_plot_with_values" width="450px" />
        </div>

        <p>
          The base TSA transformer model outperformed Llama and the Llama denseformer by ~1.6% (the denseformer didn't 
          seem to provide any performance benefits over Llama interestingly), which is not really significant enough to be an 
          interesting result in and of itself. It is interesting however in juxtaposition with the TSA denseformer. 
          The TSA Denseformer yielded an additional ~1.4% improvement over the TSA base model. That is A) a slightly 
          more significant improvement over Llama, but B) I think tells us something interesting about the denseformer architecture. 
          Linear combination with _only_ the original representation, and _only_ in the attention modules, yielded 54% of 
          the benefit of keeping some track of _every_ past representation.
        </p>
  
        <p>
          The base ORI transformer model faired better. It outperformed Llama and the Llama densformer by ~7%. I'll run 
          through some potential explanations of this result later. The ORI denseformer, on the other hand, performed worse 
          than every other TSA and ORI model. This again provides an interesting result relative to denseformers, which is 
          that non-linear interaction between the current thought and the original representation maximally (or nearly) 
          captures useful information of past representations.
        </p>

        <br />

        <h3>Rationalization</h3>
        <p>
          There are a few possible framings for why ORI performed so much better than Llama and TSA, despite being based 
          on a similar premise to TSA.
        </p>
          
        <p>
          The first is based in a concept often used in simpler statistical learning models (think anything that may be 
          used in an econometrics setting or a YouTube tutorial on SKLearn). Namely, the concept of interaction variables, 
          where there is information encoded in the interaction between two variables that is not there, or harder to identify, 
          in the raw quantities. Think `age * gender` in the Titanic dataset. You can think of the ORI structure as grounding 
          the attention of the current thought in how the last thought interacts with the original input. It seems reasonable 
          that there would be better information encoded through that process.
        </p>

        <p>
          The second is again related to a pretty fundamental concept in statistical learning, the "capacity" of a model. 
          For those unfamiliar, you can think of the capacity as the most complicated function a model can represent given 
          its parameter set. An easy example again exists in simpler models like linear regression. It is quite common 
          to increase the capacity of a linear regression model by making the model a polynomial over the features vector. 
          In this way you've kept the model linear with regards to the parameters, but have increased the set of functions 
          that the model can describe (you obviously can't describe a parabola with y = wx + b). In the case of ORI, we're 
          creating non-linearity in the embedding space to attend over while keeping the parameters fixed. This should, in 
          theory, increase the descriptiveness of the model. I didn't explore the ramifications of this conclusion, primarily 
          that this makes me think that the benefits of ORI should decrease as the parameter count and embedding dimensionality 
          increases. That is, the difference between Llama-7b and Llama-ORI-7b is probably smaller than at this ~160m parameter 
          scale because the capacity of the 7b model is already so high, thus the non-linearity is less impactful.
        </p>

        <br />

        <h3>Takeaways</h3>
        <p>
          These are pretty wide ranging and generic, and have little to do with the actual results, they're more what I 
          found tricky, interesting, and fun through the process of exploring this thesis (even though the original thesis 
          didn't play out as strongly as I would have liked).
        </p>

        <h4>Pain Points</h4>
        <p>
          - With the model architectures being custom it meant I had to do more in PyTorch relative to my usual reliance on 
          the suite of HuggingFace libraries. This was particularly tricky with the data pipeline. Creating batches was a 
          bit more of a pain than I thought, particularly with padding out sequences, and remembering to mask out padded tokens 
          during loss calculations (after I realized every model was just learning to spam "<|endoftext|>")
          - Manually combining causal and padding masks was a pain. Actually, everything to do with masking and 
          `torch.nn.functional.scaled_dot_product_attention` was a pain. Certain interactions when expanding the basic padding mask 
          input into what is needed to properly mask via softmax result in NaNs. If your data sequence is too short and you end up 
          with "-inf" values across the whole mask which results in everything being NaN from `scaled_dot_product_attention`, which 
          in turn breaks all your weights and makes every output from that point forward NaN
          - My intial thesis being outperformed by something as simple as a three line change was a bit frustrating, but I 
          suppose it's the name of the game. It would be bizarre if your first thoughts about a problem always yielded the best results
        </p>

        <h4>Interesting Points</h4>
        <p>
          - I realized pretty early on that just having loss output isn't particularly helpful. You want assurance along the 
          way that the model is actually learning language better and outputting better sentences (and not hacking the face you 
          forgot to mask out padding tokens and spamming "<|endoftext|>"). I added a generation step during my eval phases which I 
          found to be super helpful relative to the black box of usual training loops
        </p>
        
        <h4>Fun</h4>
        <p>
          - It was extremely cool watching the model learn various linguistic mechanics over the course of its training. It was 
          particularly cool to see the order of that learning. For example the fact that it picks up continuity of characters 
          before subject-pronoun agreement
          - It was also fun just working through those pain points and ending up being deeper in the weeds on a lot more of the 
          stack vs blindly relying on the HuggingFace libraries
        </p>

      </span>
    </div>
  </body>
</html>